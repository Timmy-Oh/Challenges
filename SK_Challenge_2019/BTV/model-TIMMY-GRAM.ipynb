{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import _pickle as pkl\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./temp/train.pkl', 'rb') as f:\n",
    "    df_train = pkl.load(f)\n",
    "with open('./temp/valid.pkl', 'rb') as f:\n",
    "    df_valid = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([np.arange(10981)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.transform([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train = np.array(df_train[:, 0].tolist())\n",
    "train_label = df_train[:,1]\n",
    "# train_label = [[li[0]] for li in train_label]\n",
    "train_movie = np_train[:, :, 1]\n",
    "train_dur = np_train[:, :, 2]\n",
    "train_y= mlb.transform(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_valid = np.array(df_valid[:, 0].tolist())\n",
    "valid_label = df_valid[:,1]\n",
    "# valid_label = [[li[0]] for li in valid_label]\n",
    "valid_movie = np_valid[:, :, 1]\n",
    "valid_dur = np_valid[:, :, 2]\n",
    "valid_y= mlb.transform(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_breaker(date_series):\n",
    "    ys, ms, ds ,ws = [], [], [], []\n",
    "    for i, dates in enumerate(date_series):\n",
    "        if i % 5000 == 0:\n",
    "            print(\"\\r {}/{}\".format(i, len(date_series)), end='')\n",
    "        y, m, d, w =[], [], [], []\n",
    "        for date in dates:\n",
    "            s_datetime = datetime.strptime(str(date), '%Y%m%d')\n",
    "            y.append(s_datetime.year)\n",
    "            m.append(s_datetime.month)\n",
    "            d.append(s_datetime.day)\n",
    "            w.append(s_datetime.weekday())\n",
    "        ys.append(y)\n",
    "        ms.append(m)\n",
    "        ds.append(d)\n",
    "        ws.append(w)\n",
    "    return np.array([ys, ms, ds, ws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30000/34080"
     ]
    }
   ],
   "source": [
    "train_yea, train_mon, train_day, train_wee = date_breaker(np_train[:, :, 3])\n",
    "valid_yea, valid_mon, valid_day, valid_wee = date_breaker(np_valid[:, :, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#===============keras ==============\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, concatenate\n",
    "from keras.layers import CuDNNLSTM, CuDNNGRU, Bidirectional\n",
    "from keras.layers import Dropout, SpatialDropout1D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y * 20\n",
    "valid_y = valid_y * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_size = 80\n",
    "movie_dim = 10981\n",
    "# movie_emb = 50\n",
    "movie_emb_size = 100\n",
    "dropout_rate = 0.3\n",
    "# filter_size=128\n",
    "# kernel_size = 2\n",
    "# stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Lambda\n",
    "def expand_dims(x):\n",
    "    return K.expand_dims(x, -1)\n",
    "\n",
    "def expand_dims_output_shape(input_shape):\n",
    "    return (input_shape[0], 1, input_shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "inp_mov = Input(shape=(10, ), dtype='int16', name='input_movie')\n",
    "inp_dur = Input(shape=(10, ), dtype='float32', name='input_duration')\n",
    "inp_yea = Input(shape=(10, ), dtype='int16', name='input_year')\n",
    "inp_mon = Input(shape=(10, ), dtype='int16', name='input_month')\n",
    "inp_day = Input(shape=(10, ), dtype='int16', name='input_day')\n",
    "inp_wee = Input(shape=(10, ), dtype='int16', name='input_week')\n",
    "\n",
    "idx_yea = Lambda(lambda x: x - 2017)(inp_yea)\n",
    "\n",
    "emb_year = Embedding(3, 5, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(idx_yea)\n",
    "emb_month = Embedding(13, 20, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(inp_mon)\n",
    "emb_day = Embedding(32, 5, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(inp_day)\n",
    "emb_week = Embedding(8, 10, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(inp_wee)\n",
    "\n",
    "emb_movie = Embedding(movie_dim, movie_emb_size, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(inp_mov)\n",
    "emb_dur = Lambda(expand_dims)(inp_dur)\n",
    "\n",
    "concat_input = concatenate([emb_movie, emb_dur, emb_year, emb_month, emb_day, emb_week])\n",
    "concat_input = SpatialDropout1D(rate = dropout_rate)(concat_input)\n",
    "\n",
    "x1 = Bidirectional(CuDNNLSTM(cell_size, return_sequences=True))(concat_input)\n",
    "x1 = Bidirectional(CuDNNLSTM(cell_size, return_sequences=True))(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_x1 = Lambda(lambda x:x[:, -1, :])(x1)\n",
    "\n",
    "# x1 = Conv1D(filter_size, kernel_size = kernel_size, strides=stride, padding = \"valid\", kernel_initializer = \"he_uniform\")(x1)\n",
    "avg_pool = GlobalAveragePooling1D()(x1)\n",
    "max_pool = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "##merge\n",
    "conc = concatenate([avg_pool, max_pool, last_x1])\n",
    "\n",
    "outp = Dense(2048, activation=\"relu\")(conc)\n",
    "outp = Dense(movie_dim, activation=\"sigmoid\")(conc)\n",
    "outp = Lambda(lambda x: x * 20.0)(outp) \n",
    "\n",
    "model = Model(inputs=[inp_mov, inp_dur, inp_yea, inp_mon, inp_day, inp_wee], outputs=outp)\n",
    "model.compile(optimizer=optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.002), \n",
    "              loss='mean_squared_error', loss_weights=[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./saved/best_model.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_score(y_true, y_pred, topK):\n",
    "    pred_sort = np.argsort(y_pred)\n",
    "    \n",
    "    for k in topK:\n",
    "        eval_score = 0\n",
    "        for y, y_ in zip(y_true, pred_sort):\n",
    "            eval_score += (sum(np.isin(y_[-k:], y))/k)\n",
    "        eval_score /= len(pred_sort)\n",
    "\n",
    "        print(\"MAP - top %d - score: %.6f\" % (k, eval_score))\n",
    "\n",
    "    return eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-354e131f7a15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mCustom_Eval_MAP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Callback' is not defined"
     ]
    }
   ],
   "source": [
    "class Custom_Eval_MAP(Callback):\n",
    "    def __init__(self, validation_data=(), check_k=[1,2,3,5,10,20,30], interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = map_score(self.y_val, y_pred, check_k)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_k = [1,2,3,5,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 101\n",
    "batch_size= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [train_movie, train_dur, train_yea, train_mon, train_day, train_wee]\n",
    "valid_x = [valid_movie, valid_dur, valid_yea, valid_mon, valid_day, valid_wee]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_eval = Custom_Eval_MAP(validation_data=(valid_x, valid_label), check_k= check_k, interval=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 193118 samples, validate on 34080 samples\n",
      "Epoch 1/101\n",
      "193118/193118 [==============================] - 40s 208us/step - loss: 145.3364 - val_loss: 101.6424\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 101.64239, saving model to ./saved/best_model.hdf5\n",
      "MAP - top 1 - score: 0.383392\n",
      "MAP - top 2 - score: 0.349516\n",
      "MAP - top 3 - score: 0.337109\n",
      "MAP - top 5 - score: 0.310763\n",
      "MAP - top 15 - score: 0.239644\n",
      "Epoch 2/101\n",
      "193118/193118 [==============================] - 39s 201us/step - loss: 100.6163 - val_loss: 99.2138\n",
      "\n",
      "Epoch 00002: val_loss improved from 101.64239 to 99.21385, saving model to ./saved/best_model.hdf5\n",
      "Epoch 3/101\n",
      "193118/193118 [==============================] - 38s 198us/step - loss: 98.6723 - val_loss: 97.7385\n",
      "\n",
      "Epoch 00003: val_loss improved from 99.21385 to 97.73854, saving model to ./saved/best_model.hdf5\n",
      "Epoch 4/101\n",
      "193118/193118 [==============================] - 37s 193us/step - loss: 97.6739 - val_loss: 97.0053\n",
      "\n",
      "Epoch 00004: val_loss improved from 97.73854 to 97.00529, saving model to ./saved/best_model.hdf5\n",
      "Epoch 5/101\n",
      "193118/193118 [==============================] - 39s 201us/step - loss: 96.9567 - val_loss: 96.5097\n",
      "\n",
      "Epoch 00005: val_loss improved from 97.00529 to 96.50972, saving model to ./saved/best_model.hdf5\n",
      "Epoch 6/101\n",
      "193118/193118 [==============================] - 37s 194us/step - loss: 96.5016 - val_loss: 96.2088\n",
      "\n",
      "Epoch 00006: val_loss improved from 96.50972 to 96.20876, saving model to ./saved/best_model.hdf5\n",
      "Epoch 7/101\n",
      "193118/193118 [==============================] - 38s 197us/step - loss: 96.1686 - val_loss: 95.9294\n",
      "\n",
      "Epoch 00007: val_loss improved from 96.20876 to 95.92940, saving model to ./saved/best_model.hdf5\n",
      "Epoch 8/101\n",
      "193118/193118 [==============================] - 129s 667us/step - loss: 95.8945 - val_loss: 95.8671\n",
      "\n",
      "Epoch 00008: val_loss improved from 95.92940 to 95.86712, saving model to ./saved/best_model.hdf5\n",
      "MAP - top 1 - score: 0.561092\n",
      "MAP - top 2 - score: 0.524751\n",
      "193118/193118 [==============================] - 49s 254us/step - loss: 95.6774 - val_loss: 95.6123\n",
      "\n",
      "Epoch 00009: val_loss improved from 95.86712 to 95.61234, saving model to ./saved/best_model.hdf5\n",
      "Epoch 10/101\n",
      "193118/193118 [==============================] - 40s 205us/step - loss: 95.4723 - val_loss: 95.5493\n",
      "\n",
      "Epoch 00010: val_loss improved from 95.61234 to 95.54926, saving model to ./saved/best_model.hdf5\n",
      "Epoch 11/101\n",
      "193118/193118 [==============================] - 38s 197us/step - loss: 95.3031 - val_loss: 95.4727\n",
      "\n",
      "Epoch 00011: val_loss improved from 95.54926 to 95.47266, saving model to ./saved/best_model.hdf5\n",
      "Epoch 12/101\n",
      "193118/193118 [==============================] - 38s 196us/step - loss: 95.1489 - val_loss: 95.4016\n",
      "\n",
      "Epoch 00012: val_loss improved from 95.47266 to 95.40157, saving model to ./saved/best_model.hdf5\n",
      "Epoch 13/101\n",
      "193118/193118 [==============================] - 38s 197us/step - loss: 95.0072 - val_loss: 95.4068\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 95.40157\n",
      "Epoch 14/101\n",
      "193118/193118 [==============================] - 38s 198us/step - loss: 94.8752 - val_loss: 95.3912\n",
      "\n",
      "Epoch 00014: val_loss improved from 95.40157 to 95.39122, saving model to ./saved/best_model.hdf5\n",
      "Epoch 15/101\n",
      "193118/193118 [==============================] - 38s 197us/step - loss: 94.7502 - val_loss: 95.3861\n",
      "\n",
      "Epoch 00015: val_loss improved from 95.39122 to 95.38609, saving model to ./saved/best_model.hdf5\n",
      "MAP - top 5 - score: 0.467295\n",
      "MAP - top 15 - score: 0.351847\n",
      "Epoch 16/101\n",
      "193118/193118 [==============================] - 38s 197us/step - loss: 94.6248 - val_loss: 95.3630\n",
      "\n",
      "Epoch 00016: val_loss improved from 95.38609 to 95.36298, saving model to ./saved/best_model.hdf5\n",
      "Epoch 17/101\n",
      "193118/193118 [==============================] - 38s 198us/step - loss: 94.5035 - val_loss: 95.4493\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 95.36298\n",
      "Epoch 18/101\n",
      "193118/193118 [==============================] - 38s 197us/step - loss: 94.3927 - val_loss: 95.3872\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 95.36298\n",
      "Epoch 19/101\n",
      "193118/193118 [==============================] - 38s 195us/step - loss: 94.2735 - val_loss: 95.4402\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 95.36298\n",
      "Epoch 20/101\n",
      "193118/193118 [==============================] - 38s 195us/step - loss: 94.1549 - val_loss: 95.4724\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 95.36298\n",
      "Epoch 21/101\n",
      "193118/193118 [==============================] - 38s 197us/step - loss: 94.0534 - val_loss: 95.5179\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 95.36298\n",
      "Epoch 22/101\n",
      " 56960/193118 [=======>......................] - ETA: 24s - loss: 93.7088"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-629b1c583b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m hist = model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(valid_x, valid_y),\n\u001b[1;32m----> 2\u001b[1;33m                  callbacks = [check_point, MAP_eval])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(valid_x, valid_y),\n",
    "                 callbacks = [check_point, MAP_eval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss : 95.43 -> 57.9%\n",
    "loss : 95.23 -> 58.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34080/34080 [==============================] - 3s 75us/step\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(file_path)\n",
    "pred = model.predict(valid_x, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eval(pred, y_true, movie_log, threshold = 0.5, lastK= 1, collect_argmax=True, collect_threshold = True, collect_lastlog = True, unique_y = True):\n",
    "    df_pred = pd.DataFrame(np.argwhere(pred > threshold), columns=['y_idx', 'y_pred'])\n",
    "    gb = df_pred.groupby('y_idx')\n",
    "    eval_score = 0\n",
    "    for i, y in enumerate(y_true):\n",
    "        if collect_threshold and collect_lastlog and collect_argmax:\n",
    "            try:\n",
    "                y_candi = np.concatenate([gb.get_group(i).y_pred.values, movie_log[i][-lastK:], [np.argmax(pred[i], axis=-1)]])\n",
    "            except:\n",
    "                y_candi = np.concatenate([movie_log[i][-lastK:], [np.argmax(pred[i], axis=-1)]])\n",
    "        elif (not collect_threshold) and (not collect_lastlog) and collect_argmax:\n",
    "            y_candi = np.concatenate([[np.argmax(pred[i], axis=-1)]])\n",
    "        elif (not collect_threshold) and (collect_lastlog) and collect_argmax:\n",
    "            y_candi = np.concatenate([movie_log[i][-lastK:], [np.argmax(pred[i], axis=-1)]])\n",
    "        elif collect_threshold and (not collect_lastlog) and collect_argmax:\n",
    "            try:\n",
    "                y_candi = np.concatenate([gb.get_group(i).y_pred.values, [np.argmax(pred[i], axis=-1)]])\n",
    "            except:\n",
    "                y_candi = np.concatenate([[np.argmax(pred[i], axis=-1)]])\n",
    "        elif collect_lastlog:\n",
    "            y_candi = np.concatenate([movie_log[i][-lastK:]])\n",
    "        else:\n",
    "            print('error')\n",
    "\n",
    "        if unique_y:\n",
    "            y_candi = np.unique(y_candi)\n",
    "\n",
    "        eval_score += (sum(np.isin(y_candi, y))/len(y_candi))\n",
    "    eval_score /= len(y_true)\n",
    "\n",
    "    print(\"MAP - threshold %f - last %d - score: %.6f\" % (threshold, lastK, eval_score))\n",
    "\n",
    "    return eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP - threshold 18.000000 - last 2 - score: 0.432492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4324921752738983"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_eval(pred, valid_label, valid_movie, threshold = 0.9*20, lastK= 2, collect_threshold = True, collect_lastlog = True, collect_argmax=True, unique_y = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP - top 1 - score: 0.582805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5828051643192488"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_k = [1]\n",
    "map_score(y_true=valid_label, y_pred=pred, topK=check_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K=20 : 0.184\n",
    "### k=50 : 0.132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mse 0.00259\n",
    "1 0.33215962441314556\n",
    "2 0.2771713615023474\n",
    "3 0.27060837245695274\n",
    "4 0.25244278169014084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP - top 1 - score: 0.551086\n",
    "MAP - top 2 - score: 0.513556\n",
    "MAP - top 3 - score: 0.486160\n",
    "MAP - top 5 - score: 0.444836\n",
    "MAP - top 10 - score: 0.379953"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./input/SKB_DLP_QUESTION.csv')\n",
    "\n",
    "test_users = df_test.USER_ID.unique()\n",
    "\n",
    "df_test = df_test.values.reshape([len(test_users), 10,5])\n",
    "\n",
    "test_movie = df_test[:, :, 1]\n",
    "test_dur = df_test[:, :, 2]\n",
    "\n",
    "test_x = [test_movie, test_dur]\n",
    "\n",
    "model.load_weights(file_path)\n",
    "test_pred = model.predict(test_x, batch_size=batch_size, verbose=1)\n",
    "\n",
    "subm=pd.DataFrame(test_users)\n",
    "\n",
    "subm['ans'] = np.argmax(test_pred, axis=1)\n",
    "\n",
    "subm.to_csv('./subm_test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.39366"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
