{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import _pickle as pkl\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import gc\n",
    "\n",
    "from utils import *\n",
    "\n",
    "#===============keras ==============\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Dropout\n",
    "from keras.layers import CuDNNLSTM, CuDNNGRU, Bidirectional\n",
    "from keras.layers import Dropout, SpatialDropout1D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop, Nadam\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "def expand_dims(x):\n",
    "    return K.expand_dims(x, -1)\n",
    "\n",
    "def expand_dims_output_shape(input_shape):\n",
    "    return (input_shape[0], 1, input_shape[1])\n",
    "\n",
    "def ready_data(df_train, train_label, mlb): \n",
    "    np_train = np.array(df_train)\n",
    "    train_movie = np_train[ :, :, 1]\n",
    "    train_dur = np_train[:, :, 2]\n",
    "    train_date = np_train[:, :, 3]\n",
    "    train_seq = np_train[:, :, 4]\n",
    "    train_yea, train_mon, train_day, train_wee = date_breaker(train_date)\n",
    "    train_y= mlb.transform(train_label)\n",
    "    \n",
    "    return train_movie, train_dur, train_seq, train_yea, train_mon, train_day, train_wee, train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride='hexa5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([np.arange(10981)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(k):\n",
    "    with open('./temp/train_slide_last_{}_{}.pkl'.format(stride, k), 'rb') as f:\n",
    "        df_train_x, train_label = pkl.load(f)\n",
    "    with open('./temp/valid_slide_last_{}_{}.pkl'.format(stride, k), 'rb') as f:\n",
    "        df_valid_x, valid_label = pkl.load(f)\n",
    "\n",
    "    train_movie, train_dur, train_seq, train_yea, train_mon, train_day, train_wee, train_y = ready_data(df_train_x, train_label, mlb)\n",
    "    valid_movie, valid_dur, valid_seq, valid_yea, valid_mon, valid_day, valid_wee, valid_y = ready_data(df_valid_x, valid_label, mlb)\n",
    "\n",
    "    del df_train_x, df_valid_x\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    cell_size = [120, 100, 60]\n",
    "    movie_dim = 10981\n",
    "    movie_emb_size = 80\n",
    "    dropout_rate = 0.3\n",
    "\n",
    "\n",
    "    K.clear_session()\n",
    "    inp_mov = Input(shape=(10, ), dtype='int16', name='input_movie')\n",
    "    inp_dur = Input(shape=(10, ), dtype='float32', name='input_duration')\n",
    "    inp_yea = Input(shape=(10, ), dtype='int16', name='input_year')\n",
    "    inp_mon = Input(shape=(10, ), dtype='int16', name='input_month')\n",
    "    inp_day = Input(shape=(10, ), dtype='int16', name='input_day')\n",
    "    inp_wee = Input(shape=(10, ), dtype='int16', name='input_week')\n",
    "    inp_seq = Input(shape=(10, ), dtype='int16', name='input_seq')\n",
    "\n",
    "    idx_yea = Lambda(lambda x: x - 2017)(inp_yea)\n",
    "\n",
    "    emb_movie = Embedding(movie_dim+1, movie_emb_size, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(inp_mov)\n",
    "    emb_dur = Lambda(expand_dims)(inp_dur)\n",
    "\n",
    "    emb_year = Embedding(3, 2, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(idx_yea)\n",
    "    emb_month = Embedding(13, 5, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(inp_mon)\n",
    "    emb_day = Embedding(32, 7, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(inp_day)\n",
    "    emb_week = Embedding(8, 3, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(inp_wee)\n",
    "    emb_seq = Embedding(21, 5, embeddings_initializer='he_uniform', mask_zero=False, input_length=10)(inp_seq)\n",
    "\n",
    "    concat_input = concatenate([emb_movie, emb_dur, emb_year, emb_month, emb_day, emb_week, emb_seq])\n",
    "    concat_input = SpatialDropout1D(rate = dropout_rate)(concat_input)\n",
    "\n",
    "    x1 = Bidirectional(CuDNNLSTM(cell_size[0], return_sequences=True))(concat_input)\n",
    "    x1 = Bidirectional(CuDNNLSTM(cell_size[1], return_sequences=True))(x1)\n",
    "    # x1 = Bidirectional(CuDNNLSTM(cell_size[2], return_sequences=True))(x1)\n",
    "\n",
    "    avg_pool = GlobalAveragePooling1D()(x1)\n",
    "    max_pool = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "    ##merge\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "\n",
    "    outp = Dense(2048, activation=\"relu\")(conc)\n",
    "    outp = Dropout(rate= dropout_rate)\n",
    "    outp = Dense(movie_dim, activation=\"sigmoid\")(conc)\n",
    "\n",
    "    model = Model(inputs=[inp_mov, inp_dur, inp_yea, inp_mon, inp_day, inp_wee, inp_seq], outputs=outp)\n",
    "    model.compile(optimizer=Adam(lr=0.0005), \n",
    "                  loss='mean_squared_error', loss_weights=[100])\n",
    "\n",
    "    epochs = 30\n",
    "    batch_size= 256\n",
    "\n",
    "    file_path = \"./saved/best_model_{}_{}.hdf5\".format(stride, k)\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "\n",
    "    train_x = [train_movie, train_dur, train_yea, train_mon, train_day, train_wee, train_seq]\n",
    "    valid_x = [valid_movie, valid_dur, valid_yea, valid_mon, valid_day, valid_wee, valid_seq]\n",
    "\n",
    "    check_k = [1,2,3,5]\n",
    "    MAP_eval = Custom_Eval_MAP(validation_data=(valid_x, valid_label), check_k= check_k, interval=5)\n",
    "\n",
    "    hist = model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(valid_x, valid_y),\n",
    "                     callbacks = [check_point, MAP_eval], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55000/56800WARNING:tensorflow:From C:\\Users\\dhzns\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\dhzns\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\dhzns\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\dhzns\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1016843 samples, validate on 56800 samples\n",
      "Epoch 1/50\n",
      " - 296s - loss: 0.2675 - val_loss: 0.2544\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25437, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "MAP - top 1 - score: 0.409225\n",
      "MAP - top 2 - score: 0.365053\n",
      "MAP - top 3 - score: 0.344472\n",
      "MAP - top 5 - score: 0.323835\n",
      "Epoch 2/50\n",
      " - 377s - loss: 0.1903 - val_loss: 0.2455\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25437 to 0.24554, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 3/50\n",
      " - 484s - loss: 0.1858 - val_loss: 0.2420\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24554 to 0.24203, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 4/50\n",
      " - 509s - loss: 0.1840 - val_loss: 0.2407\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.24203 to 0.24071, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 5/50\n",
      " - 557s - loss: 0.1831 - val_loss: 0.2396\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.24071 to 0.23960, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 6/50\n",
      " - 563s - loss: 0.1824 - val_loss: 0.2390\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23960 to 0.23898, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "MAP - top 1 - score: 0.576180\n",
      "MAP - top 2 - score: 0.536180\n",
      "MAP - top 3 - score: 0.505939\n",
      "MAP - top 5 - score: 0.461989\n",
      "Epoch 7/50\n",
      " - 502s - loss: 0.1818 - val_loss: 0.2384\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.23898 to 0.23842, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 8/50\n",
      " - 596s - loss: 0.1814 - val_loss: 0.2381\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23842 to 0.23810, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 9/50\n",
      " - 610s - loss: 0.1811 - val_loss: 0.2380\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23810 to 0.23797, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 10/50\n",
      " - 636s - loss: 0.1808 - val_loss: 0.2377\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.23797 to 0.23770, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 11/50\n",
      " - 626s - loss: 0.1806 - val_loss: 0.2375\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.23770 to 0.23747, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "MAP - top 1 - score: 0.595352\n",
      "MAP - top 2 - score: 0.553794\n",
      "MAP - top 3 - score: 0.521426\n",
      "MAP - top 5 - score: 0.475507\n",
      "Epoch 12/50\n",
      " - 517s - loss: 0.1804 - val_loss: 0.2375\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.23747 to 0.23746, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 13/50\n",
      " - 635s - loss: 0.1802 - val_loss: 0.2373\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.23746 to 0.23727, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 14/50\n",
      " - 618s - loss: 0.1801 - val_loss: 0.2371\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.23727 to 0.23707, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 15/50\n",
      " - 630s - loss: 0.1799 - val_loss: 0.2371\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.23707\n",
      "Epoch 16/50\n",
      " - 622s - loss: 0.1798 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.23707 to 0.23703, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "MAP - top 1 - score: 0.602658\n",
      "MAP - top 2 - score: 0.558671\n",
      "MAP - top 3 - score: 0.526197\n",
      "MAP - top 5 - score: 0.479060\n",
      "Epoch 17/50\n",
      " - 540s - loss: 0.1797 - val_loss: 0.2371\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.23703\n",
      "Epoch 18/50\n",
      " - 649s - loss: 0.1796 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.23703 to 0.23700, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "Epoch 19/50\n",
      " - 657s - loss: 0.1795 - val_loss: 0.2372\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.23700\n",
      "Epoch 20/50\n",
      " - 643s - loss: 0.1794 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.23700\n",
      "Epoch 21/50\n",
      " - 638s - loss: 0.1793 - val_loss: 0.2369\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.23700 to 0.23691, saving model to ./saved/best_model_hexa5_0.hdf5\n",
      "MAP - top 1 - score: 0.602923\n",
      "MAP - top 2 - score: 0.559974\n",
      "MAP - top 3 - score: 0.528151\n",
      "MAP - top 5 - score: 0.482158\n",
      "Epoch 22/50\n",
      " - 539s - loss: 0.1792 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.23691\n",
      "Epoch 23/50\n",
      " - 661s - loss: 0.1791 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.23691\n",
      "Epoch 24/50\n",
      " - 683s - loss: 0.1790 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.23691\n",
      "Epoch 25/50\n",
      " - 669s - loss: 0.1790 - val_loss: 0.2371\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.23691\n",
      "Epoch 26/50\n",
      " - 670s - loss: 0.1789 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.23691\n",
      "MAP - top 1 - score: 0.600863\n",
      "MAP - top 2 - score: 0.559401\n",
      "MAP - top 3 - score: 0.529096\n",
      "MAP - top 5 - score: 0.481870\n",
      "Epoch 27/50\n",
      " - 561s - loss: 0.1788 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.23691\n",
      "Epoch 28/50\n",
      " - 672s - loss: 0.1788 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.23691\n",
      "Epoch 29/50\n",
      " - 679s - loss: 0.1787 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.23691\n",
      "Epoch 30/50\n",
      " - 665s - loss: 0.1786 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.23691\n",
      "Epoch 31/50\n",
      " - 668s - loss: 0.1786 - val_loss: 0.2372\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.23691\n",
      "MAP - top 1 - score: 0.601620\n",
      "MAP - top 2 - score: 0.560246\n",
      "MAP - top 3 - score: 0.528680\n",
      "MAP - top 5 - score: 0.482169\n",
      "Epoch 32/50\n",
      " - 573s - loss: 0.1785 - val_loss: 0.2371\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.23691\n",
      "Epoch 33/50\n",
      " - 678s - loss: 0.1785 - val_loss: 0.2372\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.23691\n",
      "Epoch 34/50\n",
      " - 684s - loss: 0.1784 - val_loss: 0.2371\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.23691\n",
      "Epoch 35/50\n",
      " - 680s - loss: 0.1784 - val_loss: 0.2371\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.23691\n",
      "Epoch 36/50\n",
      " - 669s - loss: 0.1783 - val_loss: 0.2371\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.23691\n",
      "MAP - top 1 - score: 0.601039\n",
      "MAP - top 2 - score: 0.559771\n",
      "MAP - top 3 - score: 0.529020\n",
      "MAP - top 5 - score: 0.482327\n",
      "Epoch 37/50\n",
      " - 568s - loss: 0.1783 - val_loss: 0.2372\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.23691\n",
      "Epoch 38/50\n",
      " - 682s - loss: 0.1782 - val_loss: 0.2373\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.23691\n",
      "Epoch 39/50\n",
      " - 674s - loss: 0.1782 - val_loss: 0.2373\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.23691\n",
      "Epoch 40/50\n",
      " - 694s - loss: 0.1781 - val_loss: 0.2372\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.23691\n",
      "Epoch 41/50\n",
      " - 686s - loss: 0.1781 - val_loss: 0.2372\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.23691\n",
      "MAP - top 1 - score: 0.603081\n",
      "MAP - top 2 - score: 0.561100\n",
      "MAP - top 3 - score: 0.529137\n",
      "MAP - top 5 - score: 0.482553\n",
      "Epoch 42/50\n",
      " - 579s - loss: 0.1780 - val_loss: 0.2373\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.23691\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-df0d202e4b1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-aa5ef1cb9323>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(k)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     hist = model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(valid_x, valid_y),\n\u001b[1;32m---> 74\u001b[1;33m                      callbacks = [check_point, MAP_eval], verbose=2)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    183\u001b[0m                         \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[1;32m--> 185\u001b[1;33m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[0;32m    186\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55000/56801WARNING:tensorflow:From C:\\Users\\dhzns\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\dhzns\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\dhzns\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\dhzns\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1016757 samples, validate on 56801 samples\n",
      "Epoch 1/30\n",
      " - 267s - loss: 0.2714 - val_loss: 0.2555\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25549, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "MAP - top 1 - score: 0.399236\n",
      "MAP - top 2 - score: 0.358427\n",
      "MAP - top 3 - score: 0.340246\n",
      "MAP - top 5 - score: 0.321248\n",
      "Epoch 2/30\n",
      " - 393s - loss: 0.1895 - val_loss: 0.2465\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25549 to 0.24649, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 3/30\n",
      " - 482s - loss: 0.1852 - val_loss: 0.2432\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24649 to 0.24319, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 4/30\n",
      " - 511s - loss: 0.1835 - val_loss: 0.2416\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.24319 to 0.24158, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 5/30\n",
      " - 545s - loss: 0.1826 - val_loss: 0.2408\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.24158 to 0.24083, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 6/30\n",
      " - 559s - loss: 0.1819 - val_loss: 0.2403\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24083 to 0.24034, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "MAP - top 1 - score: 0.575888\n",
      "MAP - top 2 - score: 0.535950\n",
      "MAP - top 3 - score: 0.506799\n",
      "MAP - top 5 - score: 0.462886\n",
      "Epoch 7/30\n",
      " - 485s - loss: 0.1815 - val_loss: 0.2397\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24034 to 0.23969, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 8/30\n",
      " - 581s - loss: 0.1811 - val_loss: 0.2394\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23969 to 0.23937, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 9/30\n",
      " - 613s - loss: 0.1808 - val_loss: 0.2390\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23937 to 0.23905, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 10/30\n",
      " - 609s - loss: 0.1805 - val_loss: 0.2389\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.23905 to 0.23889, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 11/30\n",
      " - 621s - loss: 0.1803 - val_loss: 0.2386\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.23889 to 0.23859, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "MAP - top 1 - score: 0.597102\n",
      "MAP - top 2 - score: 0.555061\n",
      "MAP - top 3 - score: 0.523389\n",
      "MAP - top 5 - score: 0.477101\n",
      "Epoch 12/30\n",
      " - 510s - loss: 0.1801 - val_loss: 0.2386\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.23859\n",
      "Epoch 13/30\n",
      " - 635s - loss: 0.1799 - val_loss: 0.2385\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.23859 to 0.23851, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 14/30\n",
      " - 641s - loss: 0.1798 - val_loss: 0.2383\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.23851 to 0.23827, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 15/30\n",
      " - 640s - loss: 0.1796 - val_loss: 0.2385\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.23827\n",
      "Epoch 16/30\n",
      " - 649s - loss: 0.1795 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.23827 to 0.23821, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "MAP - top 1 - score: 0.605341\n",
      "MAP - top 2 - score: 0.559744\n",
      "MAP - top 3 - score: 0.527761\n",
      "MAP - top 5 - score: 0.481220\n",
      "Epoch 17/30\n",
      " - 528s - loss: 0.1794 - val_loss: 0.2383\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.23821\n",
      "Epoch 18/30\n",
      " - 645s - loss: 0.1793 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.23821\n",
      "Epoch 19/30\n",
      " - 650s - loss: 0.1792 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.23821\n",
      "Epoch 20/30\n",
      " - 657s - loss: 0.1791 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.23821\n",
      "Epoch 21/30\n",
      " - 652s - loss: 0.1790 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.23821 to 0.23816, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "MAP - top 1 - score: 0.607278\n",
      "MAP - top 2 - score: 0.562015\n",
      "MAP - top 3 - score: 0.529357\n",
      "MAP - top 5 - score: 0.481840\n",
      "Epoch 22/30\n",
      " - 562s - loss: 0.1789 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.23816\n",
      "Epoch 23/30\n",
      " - 666s - loss: 0.1788 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.23816\n",
      "Epoch 24/30\n",
      " - 665s - loss: 0.1788 - val_loss: 0.2381\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.23816 to 0.23814, saving model to ./saved/best_model_hexa5_1.hdf5\n",
      "Epoch 25/30\n",
      " - 676s - loss: 0.1787 - val_loss: 0.2383\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.23814\n",
      "Epoch 26/30\n",
      " - 670s - loss: 0.1786 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.23814\n",
      "MAP - top 1 - score: 0.605746\n",
      "MAP - top 2 - score: 0.561495\n",
      "MAP - top 3 - score: 0.529932\n",
      "MAP - top 5 - score: 0.482601\n",
      "Epoch 27/30\n",
      " - 560s - loss: 0.1785 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.23814\n",
      "Epoch 28/30\n",
      " - 679s - loss: 0.1785 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.23814\n",
      "Epoch 29/30\n",
      " - 678s - loss: 0.1784 - val_loss: 0.2383\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.23814\n",
      "Epoch 30/30\n",
      " - 676s - loss: 0.1784 - val_loss: 0.2384\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.23814\n"
     ]
    }
   ],
   "source": [
    "main(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55000/56799Train on 1016918 samples, validate on 56799 samples\n",
      "Epoch 1/30\n",
      " - 306s - loss: 0.2708 - val_loss: 0.2542\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25424, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "MAP - top 1 - score: 0.410535\n",
      "MAP - top 2 - score: 0.363272\n",
      "MAP - top 3 - score: 0.348926\n",
      "MAP - top 5 - score: 0.319900\n",
      "Epoch 2/30\n",
      " - 358s - loss: 0.1900 - val_loss: 0.2466\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25424 to 0.24658, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 3/30\n",
      " - 454s - loss: 0.1857 - val_loss: 0.2428\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24658 to 0.24278, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 4/30\n",
      " - 476s - loss: 0.1838 - val_loss: 0.2411\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.24278 to 0.24106, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 5/30\n",
      " - 507s - loss: 0.1828 - val_loss: 0.2398\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.24106 to 0.23981, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 6/30\n",
      " - 506s - loss: 0.1821 - val_loss: 0.2393\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23981 to 0.23935, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "MAP - top 1 - score: 0.578795\n",
      "MAP - top 2 - score: 0.535678\n",
      "MAP - top 3 - score: 0.507198\n",
      "MAP - top 5 - score: 0.462860\n",
      "Epoch 7/30\n",
      " - 405s - loss: 0.1816 - val_loss: 0.2387\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.23935 to 0.23874, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 8/30\n",
      " - 382s - loss: 0.1812 - val_loss: 0.2385\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23874 to 0.23846, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 9/30\n",
      " - 450s - loss: 0.1809 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23846 to 0.23817, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 10/30\n",
      " - 566s - loss: 0.1806 - val_loss: 0.2380\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.23817 to 0.23800, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 11/30\n",
      " - 587s - loss: 0.1804 - val_loss: 0.2378\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.23800 to 0.23776, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "MAP - top 1 - score: 0.600856\n",
      "MAP - top 2 - score: 0.558549\n",
      "MAP - top 3 - score: 0.525766\n",
      "MAP - top 5 - score: 0.478304\n",
      "Epoch 12/30\n",
      " - 494s - loss: 0.1803 - val_loss: 0.2377\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.23776 to 0.23772, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 13/30\n",
      " - 596s - loss: 0.1801 - val_loss: 0.2376\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.23772 to 0.23764, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 14/30\n",
      " - 594s - loss: 0.1799 - val_loss: 0.2376\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.23764 to 0.23756, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 15/30\n",
      " - 610s - loss: 0.1797 - val_loss: 0.2375\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.23756 to 0.23753, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 16/30\n",
      " - 599s - loss: 0.1796 - val_loss: 0.2374\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.23753 to 0.23735, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "MAP - top 1 - score: 0.605574\n",
      "MAP - top 2 - score: 0.562572\n",
      "MAP - top 3 - score: 0.530379\n",
      "MAP - top 5 - score: 0.482357\n",
      "Epoch 17/30\n",
      " - 507s - loss: 0.1795 - val_loss: 0.2373\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.23735 to 0.23734, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 18/30\n",
      " - 609s - loss: 0.1794 - val_loss: 0.2374\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.23734\n",
      "Epoch 19/30\n",
      " - 612s - loss: 0.1793 - val_loss: 0.2374\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.23734\n",
      "Epoch 20/30\n",
      " - 617s - loss: 0.1792 - val_loss: 0.2373\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.23734 to 0.23733, saving model to ./saved/best_model_hexa5_2.hdf5\n",
      "Epoch 21/30\n"
     ]
    }
   ],
   "source": [
    "main(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(valid_x, batch_size=batch_size, verbose=1)\n",
    "custom_eval(pred, valid_label, valid_movie, threshold = 0.92, lastK= 1, collect_threshold = False, collect_lastlog = False, collect_argmax=True, unique_y = True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./input/SKB_DLP_QUESTION.csv')\n",
    "test_users = df_test.USER_ID.unique()\n",
    "df_test = df_test.values.reshape([len(test_users), 10,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_movie = df_test[:, :, 1]\n",
    "test_dur = df_test[:, :, 2]\n",
    "test_date = df_test[:, :, 3]\n",
    "test_seq = df_test[:, :, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55000/56738"
     ]
    }
   ],
   "source": [
    "test_yea, test_mon, test_day, test_wee = date_breaker(test_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = [test_movie, test_dur, test_yea, test_mon, test_day, test_wee, test_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56738/56738 [==============================] - 6s 109us/step\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights('./saved/model_full_10_11.hdf5')\n",
    "test_pred = model.predict(test_x, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm=pd.DataFrame(test_users, columns= ['user_id'])\n",
    "\n",
    "subm['movie_id'] = np.argmax(test_pred, axis=1)\n",
    "\n",
    "subm.to_csv('./subm.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
