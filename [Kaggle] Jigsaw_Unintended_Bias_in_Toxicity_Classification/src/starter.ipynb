{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import KFold\n",
    "import re, os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "dir_emb_pre_trained = \"../emb_pre_trained\"\n",
    "submission = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func for preprocessing\n",
    "def load_data_2path(emb_model,\n",
    "             filepath_train = \"../input/train.csv\", \n",
    "             filepath_test = \"../input/test.csv\", \n",
    "             embed_size = 300,\n",
    "             max_features = 100000,\n",
    "             maxlen = 100\n",
    "            ):\n",
    "\n",
    "    DOC_Column = \"comment_text\"\n",
    "    list_classes = [\"target\"]\n",
    "\n",
    "    ###load data    \n",
    "    print(\"Data is loading ...\", end='')\n",
    "    train = pd.read_csv(filepath_train)\n",
    "    test = pd.read_csv(filepath_test)\n",
    "    print(\"\\r === Data is loaded\")\n",
    "\n",
    "    list_sentences_train = train[DOC_Column].fillna('UNK').values\n",
    "    list_sentences_test = test[DOC_Column].fillna('UNK').values\n",
    "    y = train[list_classes].values\n",
    "    \n",
    "    print(\"Data is preprocessing ...\", end='')\n",
    "    preprocessed_train = list_sentences_train.tolist()\n",
    "    preprocessed_test = list_sentences_test.tolist()\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words =max_features)\n",
    "    tokenizer.fit_on_texts(preprocessed_train + preprocessed_test)\n",
    "\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(preprocessed_train)\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(preprocessed_test)\n",
    "\n",
    "    X_t_pre = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen, truncating='pre')\n",
    "#     X_t_post = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen, truncating='post')\n",
    "    \n",
    "    X_te_pre = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen, truncating='pre')\n",
    "#     X_te_post = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen, truncating='post')\n",
    "    \n",
    "    print(\"\\r === Data is preprocessed\")\n",
    "    \n",
    "#     X_t = [X_t_pre, X_t_post]\n",
    "#     X_te = [X_te_pre, X_te_post]\n",
    "\n",
    "    X_t = X_t_pre\n",
    "    X_te = X_te_pre\n",
    "    \n",
    "    print(\"Embedding Matrix is Computing ...\", end='')\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(0.001, 0.4, (nb_words, embed_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        try:\n",
    "            embedding_vector = emb_model.get(word)\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        except: \n",
    "            pass\n",
    "    print(\"\\r === Embedding Matrix is Computed\")\n",
    "\n",
    "    return X_t, y, X_te, embedding_matrix, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = load_emb_model(dir_emb_pre_trained+'/crawl-300d-2M.vec')        # FastText Embeddings\n",
    "# emb_model = load_emb_model(dir_emb_pre_trained+'/glove.twitter.27B.100d.txt')    # Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_train = \"../input/train.csv\" \n",
    "filepath_test = \"../input/test.csv\"\n",
    "\n",
    "### preprocessing parameter\n",
    "embed_size = 128\n",
    "max_features = 150000\n",
    "maxlen = 180\n",
    "\n",
    "### classes names\n",
    "list_classes = 'target'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### model parameter\n",
    "cell_size = 64                   ### Cell unit size\n",
    "cell_type_GRU = True             ### Cell Type: GRU/LSTM\n",
    "filter_size = 64\n",
    "kernel_size = 2\n",
    "stride = 1 \n",
    "\n",
    "# ### K-fold cross-validation\n",
    "k= 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=1991)\n",
    "\n",
    "### training protocol\n",
    "epochs= 8\n",
    "batch_size = 1024\n",
    "lr_s = False                        ### Use of Learning Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Data is loaded\n",
      " === Data is preprocessed\n",
      " === Embedding Matrix is Computed\n"
     ]
    }
   ],
   "source": [
    "X_tr, Y_tr, X_te, emb_matrix, tknzr = load_data_2path(emb_model, max_features = max_features, maxlen = maxlen, embed_size=embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============keras ==============\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, concatenate, Flatten, add\n",
    "from keras.layers import CuDNNLSTM, CuDNNGRU, Bidirectional\n",
    "from keras.layers import Dropout, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import MaxPooling1D, Activation\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dropout = 0.2\n",
    "emb_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_pre = Input(shape=(maxlen, ), name='input_pre')\n",
    "#     inp_post = Input(shape=(maxlen, ), name='input_post')\n",
    "\n",
    "\n",
    "##pre\n",
    "x1 = Embedding(max_features, embed_size, weights=[emb_matrix], trainable = emb_train)(inp_pre)\n",
    "x1 = SpatialDropout1D(rate = prob_dropout)(x1)\n",
    "\n",
    "if cell_type_GRU:\n",
    "    x1 = Bidirectional(CuDNNGRU(cell_size, return_sequences=True))(x1)\n",
    "else :\n",
    "    x1 = Bidirectional(CuDNNLSTM(cell_size, return_sequences=True))(x1)\n",
    "\n",
    "avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "##post\n",
    "#     x2 = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable = emb_train)(inp_post)\n",
    "#     x2 = SpatialDropout1D(prob_dropout)(x2)\n",
    "    \n",
    "#     if cell_type_GRU:\n",
    "#         x2 = Bidirectional(CuDNNGRU(cell_size, return_sequences=True))(x2)\n",
    "#     else :\n",
    "#         x2 = Bidirectional(CuDNNLSTM(cell_size, return_sequences=True))(x2)\n",
    "    \n",
    "#     avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "#     max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "conc = concatenate([avg_pool1, max_pool1])\n",
    "outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "\n",
    "model = Model(inputs=inp_pre, outputs=outp)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['mse','binary_crossentropy', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint\n",
    "            \n",
    "def schedule(ind):\n",
    "    a = [0.001, 0.0008, 0.0006, 0.0004, 0.0002, 0.0001, 0.00005, 0.003, 0.0005, 0.0001, 0.00005,\n",
    "         0.00005, 0.00005, 0.00005, 0.00005, 0.00005, 0.00005, 0.00005, 0.00005, 0.00005, 0.00005, 0.00005, 0.00005]\n",
    "    return a[ind]\n",
    "        \n",
    "def model_train_cv(model, X_tra, X_val, y_tra, y_val, x_test, model_name, batch_size = 1024, epochs = 2, lr_schedule=True):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    \n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "    lr_s = LearningRateScheduler(schedule)\n",
    "    \n",
    "    if lr_schedule:\n",
    "        hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                             callbacks = [lr_s, check_point], verbose=1)\n",
    "    else:\n",
    "        print('== no learing schedule')\n",
    "        hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                             callbacks = [check_point], verbose=1)\n",
    "        \n",
    "    model.load_weights(file_path)\n",
    "    oof = model.predict(X_val, batch_size=batch_size, verbose=1)\n",
    "    pred = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    return pred, oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== no learing schedule\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/8\n",
      "1443899/1443899 [==============================] - 93s 65us/step - loss: 0.2592 - mean_squared_error: 0.0207 - binary_crossentropy: 0.2592 - acc: 0.7006 - val_loss: 0.2376 - val_mean_squared_error: 0.0152 - val_binary_crossentropy: 0.2376 - val_acc: 0.7001\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.23759, saving model to best_model.hdf5\n",
      "Epoch 2/8\n",
      "1305600/1443899 [==========================>...] - ETA: 8s - loss: 0.2364 - mean_squared_error: 0.0150 - binary_crossentropy: 0.2364 - acc: 0.70"
     ]
    }
   ],
   "source": [
    "model_name = 'rnn'\n",
    "\n",
    "### ================================================================== ###\n",
    "oofs = []\n",
    "res = np.zeros_like(submission[\"prediction\"])\n",
    "\n",
    "for train_index, val_index in kf.split(X_tr, Y_tr):\n",
    "#     mdl = Toxic_Models.get_model_rnn(emb_matrix, cell_size=cell_size, maxlen=maxlen, cell_type_GRU=cell_type_GRU)\n",
    "    pred, oof = model_train_cv(model, X_tra = X_tr[train_index], X_val = X_tr[val_index],\n",
    "                                             y_tra=  Y_tr[train_index], y_val= Y_tr[val_index], x_test=X_te, \n",
    "                                             model_name=model_name, batch_size=batch_size, epochs=epochs, lr_schedule=lr_s)\n",
    "    res += pred\n",
    "    oofs.append(oof)\n",
    "    K.clear_session()\n",
    "    time.sleep(20)\n",
    "    \n",
    "res = res/k\n",
    "    \n",
    "\n",
    "### Collect result & Report\n",
    "submission[list_classes] = res\n",
    "submission.to_csv(\"submission_{}.csv\".format(model_name), index = False)\n",
    "\n",
    "np_oofs = np.array(oofs)\n",
    "pd_oofs = pd.DataFrame(np.concatenate(np_oofs), columns=list_classes)\n",
    "pd_oofs.to_csv(\"oofs_{}.csv\".format(model_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
